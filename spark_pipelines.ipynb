{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimizando flujos de datos con PySpark\n",
    "\n",
    "\n",
    "\n",
    "### Mónica Zamudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Qué es Apache Spark?\n",
    "\n",
    "> Motor de cómputo unificado + conjunto de bibliotecas para procesamiento distribuido de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*(términos clave: motor de cómputo, unificado, bibliotecas)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Una mirada rápida](images/spdg_0101.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algunos detalles importantes sobre Spark\n",
    "\n",
    "- Basado en Hadoop MapReduce\n",
    "\n",
    "- Implementado en Scala\n",
    "\n",
    "- APIs para varios lenguajes: Python, R, SQL, Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ¿Eso qué implica?\n",
    "\n",
    "- Paradigma funcional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Python se habla con Java\n",
    "\n",
    "![](images/sad.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Abstracciones básicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lógica de ejecución\n",
    "\n",
    "- Transformations: instrucciones para mutar estructuras de datos (withColumn, groupBy, cast, filter, etc.)\n",
    "\n",
    "- Actions: detonan la ejecución de una serie de transformaciones (collect, agg, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RDDs: Resilient Distributed Datasets\n",
    "\n",
    "- Colección particionada, inmutable y tolerante a fallas\n",
    "\n",
    "- Abstracción \"base\", permite mucho control sobre las transformaciones que hacemos en los datos\n",
    "\n",
    "- Puede procesar datos no estructurados y estructurados, pero no infiere el schema de nuestros datos estructurados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dataframes\n",
    "\n",
    "- NO son DataFrames de Pandas/R\n",
    "\n",
    "- Estructuras que representan tablas (datos estructurados/semiestructurados) de forma particionada\n",
    "\n",
    "- Implementa distintas fases de optimización antes de tener el plan de ejecución definitivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arquitectura de Spark\n",
    "\n",
    "![](images/spark_architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/spark_architecture_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ¿Eso qué implica?\n",
    "\n",
    "- Overhead (latencia) para inicializar workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Mensajes de error: Python + Java\n",
    "\n",
    "```\n",
    "20/09/29 22:32:35 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): TaskKilled (Stage cancelled) Traceback (most recent call last): File \"pyspark_job.py\", line 61, in main() File \"pyspark_job.py\", line 58, in main process_events_data(spark, input_path) File \"pyspark_job.py\", line 41, in process_events_data df = spark.read.format('json').load(input_path) File \"spark_dir\\spark-2.4.7_h2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\readwriter.py\", line 166, in load File \"spark_dir\\spark-2.4.7_h2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1257, in call File \"spark_dir\\spark-2.4.7_h2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 63, in deco File \"spark_dir\\spark-2.4.7_h2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\", line 328, in get_return_value py4j.protocol.Py4JJavaError: An error occurred while calling o42.load. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 138345560; received: 71284276\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/sad.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Doble serialización\n",
    "\n",
    "![](images/avocado.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Entonces ¿Cómo lo optimizo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Reutilización de datos\n",
    "\n",
    "Python es un lenguaje interpretado. No puede preveer que vamos a reutilizar datos después de ejecutar una primera serie de transformaciones.\n",
    "\n",
    "```\n",
    "rdd = sc.textFile('mis_datos.txt')\n",
    "palabras = rdd.flatMap(lambda x: x.split(' '))\n",
    "palabras_tuplas = palabras.map(lambda w: (w, 1))\n",
    "palabras_agrupadas = wordPairs.groupByKey()\n",
    "grouped.mapValues(lambda counts: sum(counts))\n",
    "grouped.saveAsTextFile('cuenta_palabras')\n",
    "\n",
    "errores = rdd.filter(lambda x: x.lower.find('ERROR') != -1).count()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Podemos:\n",
    "\n",
    "- Persistir los datos en memoria si no son tan grandes (`cache`)\n",
    "\n",
    "- Persistir los datos en memoria, disco o ambos (`persist`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "rdd = sc.textFile('mis_datos.txt').cache()\n",
    "palabras = rdd.flatMap(lambda x: x.split(' '))\n",
    "palabras_tuplas = palabras.map(lambda w: (w, 1))\n",
    "palabras_agrupadas = wordPairs.groupByKey()\n",
    "grouped.mapValues(lambda counts: sum(counts))\n",
    "grouped.saveAsTextFile('cuenta_palabras')\n",
    "\n",
    "errores = rdd.filter(lambda x: x.lower.find('error') != -1).count()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Agrupaciones en RDDs: ReduceByKey vs GroupByKey\n",
    "\n",
    "```\n",
    "rdd = sc.textFile('mis_datos.txt')\n",
    "palabras = rdd.flatMap(lambda x: x.split(' '))\n",
    "palabras_tuplas = palabras.map(lambda w: (w, 1))\n",
    "palabras_agrupadas = wordPairs.groupByKey()\n",
    "grouped.mapValues(lambda counts: sum(counts))\n",
    "grouped.saveAsTextFile('cuenta_palabras')\n",
    "\n",
    "errores = rdd.filter(lambda x: x.lower.find('error') != -1).count()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![GroupByKey](images/groupByKey.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![ReduceByKey](images/reduceByKey.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Utiliza DataFrames cuando sea posible, y evita las UDFs\n",
    "\n",
    "- Los DataFrames traducen código de Python directamente en un Plan de ejecución que puede ejecutarse directamente en la JVM. ¡Nos libramos de la doble serialización! Y no transferimos datos entre Python y Java\n",
    "\n",
    "- Los planes de ejecución pasan por una (o más) capa de optimización antes de ejecutarse\n",
    "\n",
    "- ¡Las UDFs (User-Defined Functions) son una caja negra para la JVM! Necesitaríamos pasarle los datos al intérprete de Python para que ejecute nuestra UDF y serialice el resultado y lo regrese a la JVM. Mantente en expresiones nativas de PySpark siempre que puedas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Optimiza el I/O de datos cuando te sea posible\n",
    "\n",
    "### Mi sugerencia: Parquet\n",
    "\n",
    "- Formato columnar distribuido de almacenamiento de datos\n",
    "\n",
    "- Compresión y codificación muy eficientes\n",
    "\n",
    "- Permite selección y particionado a partir de campos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. No hagas Full Scans innecesarios. Es en serio.\n",
    "\n",
    "Parece chiste, pero es anécdota:\n",
    "\n",
    "```\n",
    "mis_datos = spark.read.parquet('mis_datos')\n",
    "mis_datos_agrupados = mis_datos.groupBy('anio', 'mes', 'id_cliente').agg(sum('ventas'))\n",
    "\n",
    "mis_datos_cliente = mis_datos_agrupados.filter(col('id_cliente') = '001')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. Broadcast joins\n",
    "\n",
    "- Joins comunes entre dos dataframes con tamaños muy distintos.\n",
    "\n",
    "- Evita el reparticionado entre nodos de datos\n",
    "\n",
    "- Spark hace broadcast joins en la medida en que uno de los datasets es más pequeño que `spark.sql.autoBroadcastJoinThreshold` (propiedad configurable)\n",
    "\n",
    "- Se puede \"sobrepasar\" esa configuración con la función `broadcast`:\n",
    "\n",
    "```\n",
    "print(df1.join(broadcast(df2),df2.id==df1._id).take(10))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7. ¡No hagas ciclos `for`!\n",
    "\n",
    "- Al momento de hacer transformaciones en ciclos `for`, PySpark no ejecuta de forma secuencial esas transformaciones: PySpark genera una serie de planes de ejecución que compone y manda ejecutar *de forma simultánea* al momento de aplicar las acciones.\n",
    "\n",
    "- Necesitamos darle a PySpark flujos de control que pueda traducir a `map` + `reduce`. De lo contrario:\n",
    "\n",
    "```\n",
    "org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 5136 tasks (1024.0 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algunas ideas:\n",
    "\n",
    "- `rdd.map()`, `spark.sql.functions.create_map()`\n",
    "\n",
    "- Implementar esa misma lógica via joins \n",
    "\n",
    "- Para dos ciclos `for` anidados: implementación de *flat maps* en Python\n",
    "    \n",
    "    - for + extend\n",
    "    \n",
    "    - doble comprensión de listas\n",
    "    \n",
    "    - map + reduce (Python)\n",
    "    \n",
    "    - map + sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8. Prueba distintas configuraciones de número de particiones\n",
    "\n",
    "> Idealmente, queremos que las particiones correspondan (más o menos) con el tamaño y número de nodos ejecutores. Queremos que la carga de procesamiento esté balanceada entre los nodos ejecutores.\n",
    "\n",
    "- Particiones demasiado grandes no cabrán en la memoria de un nodo ejecutor\n",
    "\n",
    "- Muy pocas particiones podrían hacer que los nodos ejecutores pasen mucho tiempo sin actividad\n",
    "\n",
    "- Muchas particiones inducen mucho *overhead* de orquestación\n",
    "\n",
    "- Queremos mínimo 2x el número de núcleos en el clúster\n",
    "\n",
    "- Revisa el tiempo que las tareas toman en ejecutarse. Si cada tarea individual toma muy poco, podrías estar perdiendo mucho tiempo en *scheduling* que podría optimizarse agrandando las particiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/monitor_executors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Shameless Self-promotion\n",
    "\n",
    "## ¡Estamos contratando!\n",
    "\n",
    "![](images/opi.png)\n",
    "![](images/navidopi.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "- Spark: The Definitive Guide - Bill Chambers, Matei Zaharia. (2018) O'Reilly\n",
    "\n",
    "- Improving PySpark Performance: Spark performance beyond the JVM - Holden Karau https://www.youtube.com/watch?v=jGhju2bw3RQ&list=PLRLebp9QyZtaoIpE2iaF3Q8itJOcdgYoX&index=37&ab_channel=PyConAU\n",
    "\n",
    "- Apache Spark RDD vs DataFrame vs DataSet - Data Flair https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/\n",
    "\n",
    "- The Internals of Spark SQL - Jacek Laskowski (2020) https://the-internals-of-spark-sql.readthedocs.io/spark-sql/\n",
    "\n",
    "- Flat map in Python - Tomasz Urbaszek (2020) https://dev.to/turbaszek/flat-map-in-python-3g98\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
